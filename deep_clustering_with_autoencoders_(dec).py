# -*- coding: utf-8 -*-
"""Deep Clustering with Autoencoders (DEC).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_RjUyKMAUg8rwD--Z6YLdLdsS-x7jOag
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler,StandardScaler
from keras.layers import Input, Dense
from keras.models import Model
from keras.initializers import glorot_uniform
from keras.optimizers import SGD
from keras.losses import MeanSquaredError
import pandas as pd

# Step 1: Generate random points and weights
np.random.seed(42)  # For reproducibility
num_points = 300
x = np.random.uniform(0, 2000, num_points)
y = np.random.uniform(0, 2000, num_points)
weights = np.random.randint(1, 101, num_points)

# Visualize the original points
plt.figure(figsize=(8, 8))
plt.scatter(x, y, s=weights, alpha=0.6, edgecolors='k')
plt.title("Original Points with Weights")
plt.xlabel("X-axis (meters)")
plt.ylabel("Y-axis (meters)")
plt.show()

# Step 2: Prepare data for clustering
data = np.vstack((x, y)).T
scaler = StandardScaler() # MinMaxScaler()
data_scaled = scaler.fit_transform(data)

# Step 3: Define Autoencoder
input_dim = data_scaled.shape[1]
input_layer = Input(shape=(input_dim,))
encoder = Dense(128, activation='relu', kernel_initializer=glorot_uniform())(input_layer)
encoder = Dense(64, activation='relu')(encoder)
encoder = Dense(32, activation='relu')(encoder)
latent = Dense(10, activation='relu')(encoder)

decoder = Dense(32, activation='relu')(latent)
decoder = Dense(64, activation='relu')(decoder)
decoder = Dense(128, activation='relu')(decoder)
output_layer = Dense(input_dim, activation='sigmoid')(decoder)

autoencoder = Model(input_layer, output_layer)
autoencoder.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9), loss=MeanSquaredError())

# autoencoder.compile(optimizer="adam", loss="mse")
autoencoder.summary()

# Train Autoencoder
autoencoder.fit(data_scaled, data_scaled, epochs=50, batch_size=32, verbose=1)
# autoencoder.fit(data_normalized, data_normalized, epochs=50, batch_size=32, shuffle=True, verbose=1)

# Extract Encoder Model
encoder_model = Model(input_layer, latent)
encoded_data = encoder_model.predict(data_scaled)

# Step 4: Apply KMeans clustering on encoded data
num_clusters = 5
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
clusters = kmeans.fit_predict(encoded_data)


# Step 5: Calculate weight sums and the number of points in each cluster for each cluster
weight_sums = []
points_per_cluster = []
for cluster_id in range(num_clusters):
    cluster_weights = weights[clusters == cluster_id]
    weight_sums.append(np.sum(cluster_weights))
    num_points_in_cluster = np.sum(clusters == cluster_id)
    points_per_cluster.append(num_points_in_cluster)

# Create a table for weight sums
cluster_table = pd.DataFrame({
    'Cluster ID': range(1, num_clusters + 1),
    'Weight Sum': weight_sums,
    'Number of Points': points_per_cluster
})

# Display the table
print(cluster_table)


# Step 6: Visualize clustering results
plt.figure(figsize=(8, 8))
for cluster_id in range(num_clusters):
    cluster_points = data[clusters == cluster_id]
    cluster_weights = weights[clusters == cluster_id]
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], s=cluster_weights, alpha=0.6, label=f"Cluster {cluster_id+1}")

plt.title("Clustering Results with DEC")
plt.xlabel("X-axis (meters)")
plt.ylabel("Y-axis (meters)")
plt.legend()
plt.show()