# -*- coding: utf-8 -*-
"""F_GMM-EM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-of7LCO8jN8-GQ37_PzD20wAvSCEDA-9
"""

import numpy as np
from numpy import random
from matplotlib.patches import Ellipse
import matplotlib.transforms as transforms
from scipy.stats import multivariate_normal
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal

"""# Normal training"""

class GMM():
    def __init__(self, k, dim, init_mu=None, init_sigma=None, init_pi=None, colors=None):
        '''
        Define a model with known number of clusters and dimensions.
        input:
            - k: Number of Gaussian clusters
            - dim: Dimension
            - init_mu: initial value of mean of clusters (k, dim)
                       (default) random from uniform[-10, 10]
            - init_sigma: initial value of covariance matrix of clusters (k, dim, dim)
                          (default) Identity matrix for each cluster
            - init_pi: initial value of cluster weights (k,)
                       (default) equal value to all cluster i.e. 1/k
            - colors: Color valu for plotting each cluster (k, 3)
                      (default) random from uniform[0, 1]
        '''
        self.k = k
        self.dim = dim
        if(init_mu is None):
            init_mu = random.rand(k, dim)*20 - 10
        self.mu = init_mu
        if(init_sigma is None):
            init_sigma = np.zeros((k, dim, dim))
            for i in range(k):
                init_sigma[i] = np.eye(dim)
        self.sigma = init_sigma
        if(init_pi is None):
            init_pi = np.ones(self.k)/self.k
        self.pi = init_pi
        if(colors is None):
            colors = random.rand(k, 3)
        self.colors = colors

    def init_em(self, X, weights):
        '''
        Initialization for EM algorithm with weighted data.
        input:
            - X: Data (batch_size, dim).
            - weights: Array of weights for the data points.
        '''
        self.data = X
        self.weights = weights
        self.normalized_weights = weights / np.sum(weights) * len(weights)
        self.num_points = X.shape[0]
        self.z = np.zeros((self.num_points, self.k))
    def e_step(self):
        '''
        E-step of EM algorithm (with weights).
        '''
        for i in range(self.k):
            self.z[:, i] = self.pi[i] * multivariate_normal.pdf(self.data, mean=self.mu[i], cov=self.sigma[i])
        self.z *= self.normalized_weights[:, None]  # Weight each point's responsibility
        self.z /= self.z.sum(axis=1, keepdims=True)



    # For balanced weight constraints
    def e_step_balanced(self):
        '''
        E-step of EM algorithm with balanced weight constraints.
        '''
        avg_weight = np.sum(self.normalized_weights) / self.k  # Target balanced weight per cluster

        for i in range(self.k):
            # Compute the likelihood for each cluster
            self.z[:, i] = self.pi[i] * multivariate_normal.pdf(self.data, mean=self.mu[i], cov=self.sigma[i])

        # Adjust responsibilities to enforce balanced weights
        current_weights = np.sum(self.z * self.normalized_weights[:, None], axis=0)
        scaling_factors = avg_weight / (current_weights + 1e-9)  # Avoid division by zero
        self.z *= scaling_factors[None, :]  # Scale responsibilities
        self.z /= self.z.sum(axis=1, keepdims=True)  # Normalize


    def m_step(self):
        '''
        M-step of EM algorithm (with weights).
        '''
        weighted_z = self.z * self.normalized_weights[:, None]
        sum_z = weighted_z.sum(axis=0)
        self.pi = sum_z / np.sum(self.normalized_weights)
        self.mu = np.matmul(weighted_z.T, self.data)
        self.mu /= sum_z[:, None]
        for i in range(self.k):
            j = np.expand_dims(self.data, axis=1) - self.mu[i]
            s = np.matmul(j.transpose([0, 2, 1]), j)
            self.sigma[i] = np.matmul(s.transpose(1, 2, 0), weighted_z[:, i])
            self.sigma[i] /= sum_z[i]

    # For balanced weight constraints
    def m_step_balanced(self):
        '''
        M-step of EM algorithm with balanced weight constraints.
        '''
        weighted_z = self.z * self.normalized_weights[:, None]
        sum_z = weighted_z.sum(axis=0)

        # Force balanced cluster weights
        self.pi = np.ones(self.k) / self.k  # Equal weight for each cluster

        # Update means and covariances
        self.mu = np.matmul(weighted_z.T, self.data)
        self.mu /= sum_z[:, None]
        for i in range(self.k):
            j = np.expand_dims(self.data, axis=1) - self.mu[i]
            s = np.matmul(j.transpose([0, 2, 1]), j)
            self.sigma[i] = np.matmul(s.transpose(1, 2, 0), weighted_z[:, i])
            self.sigma[i] /= sum_z[i]



    def log_likelihood(self, X):
        '''
        Compute the log-likelihood of X under current parameters
        input:
            - X: Data (batch_size, dim)
        output:
            - log-likelihood of X: Sum_n Sum_k log(pi_k * N( X_n | mu_k, sigma_k ))
        '''
        ll = []
        for d in X:
            tot = 0
            for i in range(self.k):
                tot += self.pi[i] * multivariate_normal.pdf(d, mean=self.mu[i], cov=self.sigma[i])
            ll.append(np.log(tot))
        return np.sum(ll)

    def plot_gaussian(self, mean, cov, ax, n_std=3.0, facecolor='none', **kwargs):
        '''
        Utility function to plot one Gaussian from mean and covariance.
        '''
        pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])
        ell_radius_x = np.sqrt(1 + pearson)
        ell_radius_y = np.sqrt(1 - pearson)
        ellipse = Ellipse((0, 0),
            width=ell_radius_x * 2,
            height=ell_radius_y * 2,
            facecolor=facecolor,
            **kwargs)
        scale_x = np.sqrt(cov[0, 0]) * n_std
        mean_x = mean[0]
        scale_y = np.sqrt(cov[1, 1]) * n_std
        mean_y = mean[1]
        transf = transforms.Affine2D() \
            .rotate_deg(45) \
            .scale(scale_x, scale_y) \
            .translate(mean_x, mean_y)
        ellipse.set_transform(transf + ax.transData)
        return ax.add_patch(ellipse)

    def draw(self, ax, n_std=2.0, facecolor='none', **kwargs):
        '''
        Function to draw the Gaussians.
        Note: Only for two-dimensionl dataset
        '''
        if(self.dim != 2):
            print("Drawing available only for 2D case.")
            return
        for i in range(self.k):
            self.plot_gaussian(self.mu[i], self.sigma[i], ax, n_std=n_std, edgecolor=self.colors[i], **kwargs)

def gen_data_random_with_weights(total_points, dim=2, lim=[-20, 20]):
    '''
    Generates random data with random weights assigned to each point.
    input:
        - k: Ignored but kept for compatibility.
        - dim: Dimension of generated points.
        - points_per_cluster: Total number of points to generate.
        - lim: Range for each dimension.
    output:
        - X: Randomly distributed points (points_per_cluster*k, dim).
        - weights: Array of weights (points_per_cluster*k,).
    '''

    X = np.random.uniform(low=lim[0], high=lim[1], size=(total_points, dim))
    weights = np.random.uniform(1, 100, size=total_points)  # Random weights between 1 and 100

    fig = plt.figure()
    ax = fig.gca()
    sc = ax.scatter(X[:, 0], X[:, 1], c=weights, s=3, alpha=0.4, cmap='viridis')
    plt.colorbar(sc, label="Weights")
    ax.autoscale(enable=True)
    plt.title("Randomly Distributed Data with Weights")
    return X, weights


def plot(title):
    '''
    Draw the data points with weights represented by circle sizes and clusters with different colors.
    input:
        - title: Title of plot and name with which it will be saved.
    '''
    fig = plt.figure(figsize=(8, 8))
    ax = fig.gca()

    # Assign each point to the most likely cluster
    cluster_assignments = np.argmax(gmm.z, axis=1)

    # Plot each cluster's points in a different color
    for cluster_idx in range(gmm.k):
        cluster_points = X[cluster_assignments == cluster_idx]
        cluster_weights = weights[cluster_assignments == cluster_idx]
        ax.scatter(
            cluster_points[:, 0], cluster_points[:, 1],
            s=cluster_weights,  # Scale size by weight
            c=[gmm.colors[cluster_idx]],  # Cluster color
            alpha=0.6,
            label=f"Cluster {cluster_idx + 1}"
        )

    # Plot Gaussian ellipses
    gmm.draw(ax, n_std=2.0, lw=2)

    # Add legend and title
    ax.set_xlim((-20, 20))
    ax.set_ylim((-20, 20))
    plt.title(title)
    plt.legend()
    plt.savefig(title.replace(':', '_'))
    plt.show()
    plt.clf()



# Calculate weight sum and point count for each cluster
def summarize_clusters():
    cluster_assignments = np.argmax(gmm.z, axis=1)  # Assign each point to the most likely cluster
    cluster_summary = []

    for cluster_idx in range(gmm.k):
        cluster_points = X[cluster_assignments == cluster_idx]
        cluster_weights = weights[cluster_assignments == cluster_idx]
        cluster_summary.append({
            "Cluster": cluster_idx + 1,
            "Number of Points": len(cluster_points),
            "Weight Sum": np.sum(cluster_weights)
        })

    # Create a DataFrame for better presentation
    summary_df = pd.DataFrame(cluster_summary)

    # Print the table to the console
    print(summary_df.to_string(index=False))




# Generate random 2D data with weights
X, weights = gen_data_random_with_weights(300, dim=2, lim=[-20, 20])

######## Normal training

# Create a Gaussian Mixture Model
gmm = GMM(5, 2)

# Initialize EM algorithm with weighted data
gmm.init_em(X, weights)
num_iters = 10

# Saving log-likelihood
log_likelihood = [gmm.log_likelihood(X)]

# Plot initial data distribution
plot("Iteration:  0")
for e in range(num_iters):
    # E-step
    gmm.e_step()
    # M-step
    gmm.m_step()
    # Compute log-likelihood
    log_likelihood.append(gmm.log_likelihood(X))
    print("Iteration: {}, log-likelihood: {:.4f}".format(e + 1, log_likelihood[-1]))
    # Plot updated clusters
    plot(title="Iteration: " + str(e + 1))
# Call the function to display the summary
summarize_clusters()

"""# Balanced training"""

## Balanced training

# # Generate random 2D data with weights and 5 clusters
# X, weights = gen_data_random_with_weights(300, dim=2, lim=[-20, 20])


# Create a Gaussian Mixture Model with 5 clusters
gmm = GMM(k=5, dim=2)

# Initialize EM algorithm with weighted data
gmm.init_em(X, weights)

# Training with balanced EM
num_iters = 10
log_likelihood = [gmm.log_likelihood(X)]
plot("Iteration:  0")
for e in range(num_iters):
    gmm.e_step_balanced()  # Use balanced E-step
    gmm.m_step_balanced()  # Use balanced M-step
    log_likelihood.append(gmm.log_likelihood(X))
    print("Iteration: {}, log-likelihood: {:.4f}".format(e + 1, log_likelihood[-1]))
    plot(title="Iteration: " + str(e + 1))

# Summarize the clusters
summarize_clusters()



"""# **Now the want the shape to be spherical:**

"""

class GMM():
    def __init__(self, k, dim, init_mu=None, init_sigma=None, init_pi=None, colors=None):
        '''
        Define a model with known number of clusters and dimensions.
        input:
            - k: Number of Gaussian clusters
            - dim: Dimension
            - init_mu: initial value of mean of clusters (k, dim)
                       (default) random from uniform[-10, 10]
            - init_sigma: initial value of covariance matrix of clusters (k, dim, dim)
                          (default) Identity matrix for each cluster
            - init_pi: initial value of cluster weights (k,)
                       (default) equal value to all cluster i.e. 1/k
            - colors: Color valu for plotting each cluster (k, 3)
                      (default) random from uniform[0, 1]
        '''
        self.k = k
        self.dim = dim
        if(init_mu is None):
            init_mu = random.rand(k, dim)*20 - 10
        self.mu = init_mu
        if(init_sigma is None):
            init_sigma = np.zeros((k, dim, dim))
            for i in range(k):
                init_sigma[i] = np.eye(dim)
        self.sigma = init_sigma
        if(init_pi is None):
            init_pi = np.ones(self.k)/self.k
        self.pi = init_pi
        if(colors is None):
            colors = random.rand(k, 3)
        self.colors = colors

    def init_em(self, X, weights):
        '''
        Initialization for EM algorithm with weighted data.
        input:
            - X: Data (batch_size, dim).
            - weights: Array of weights for the data points.
        '''
        self.data = X
        self.weights = weights
        self.normalized_weights = weights / np.sum(weights) * len(weights)
        self.num_points = X.shape[0]
        self.z = np.zeros((self.num_points, self.k))
    def e_step(self):
        '''
        E-step of EM algorithm (with weights).
        '''
        for i in range(self.k):
            self.z[:, i] = self.pi[i] * multivariate_normal.pdf(self.data, mean=self.mu[i], cov=self.sigma[i])
        self.z *= self.normalized_weights[:, None]  # Weight each point's responsibility
        self.z /= self.z.sum(axis=1, keepdims=True)



    # For balanced weight constraints
    def e_step_balanced(self):
        '''
        E-step of EM algorithm with balanced weight constraints.
        '''
        avg_weight = np.sum(self.normalized_weights) / self.k  # Target balanced weight per cluster

        for i in range(self.k):
            # Compute the likelihood for each cluster
            self.z[:, i] = self.pi[i] * multivariate_normal.pdf(self.data, mean=self.mu[i], cov=self.sigma[i])

        # Adjust responsibilities to enforce balanced weights
        current_weights = np.sum(self.z * self.normalized_weights[:, None], axis=0)
        scaling_factors = avg_weight / (current_weights + 1e-9)  # Avoid division by zero
        self.z *= scaling_factors[None, :]  # Scale responsibilities
        self.z /= self.z.sum(axis=1, keepdims=True)  # Normalize






    def m_step(self):
        '''
        M-step of EM algorithm (with weights).
        '''
        weighted_z = self.z * self.normalized_weights[:, None]
        sum_z = weighted_z.sum(axis=0)
        self.pi = sum_z / np.sum(self.normalized_weights)
        self.mu = np.matmul(weighted_z.T, self.data)
        self.mu /= sum_z[:, None]
        for i in range(self.k):
            j = np.expand_dims(self.data, axis=1) - self.mu[i]
            s = np.matmul(j.transpose([0, 2, 1]), j)
            self.sigma[i] = np.matmul(s.transpose(1, 2, 0), weighted_z[:, i])
            self.sigma[i] /= sum_z[i]

    # For balanced weight constraints
    def m_step_balanced(self):
        '''
        M-step of EM algorithm with balanced weight constraints.
        '''
        weighted_z = self.z * self.normalized_weights[:, None]
        sum_z = weighted_z.sum(axis=0)

        # Force balanced cluster weights
        self.pi = np.ones(self.k) / self.k  # Equal weight for each cluster

        # Update means and covariances
        self.mu = np.matmul(weighted_z.T, self.data)
        self.mu /= sum_z[:, None]
        for i in range(self.k):
            j = np.expand_dims(self.data, axis=1) - self.mu[i]
            s = np.matmul(j.transpose([0, 2, 1]), j)
            self.sigma[i] = np.matmul(s.transpose(1, 2, 0), weighted_z[:, i])
            self.sigma[i] /= sum_z[i]

    # For balanced and spherical weight constraints
    def m_step_balanced_spherical(self):
        '''
        M-step of EM algorithm with balanced weight constraints and spherical clusters.
        '''
        weighted_z = self.z * self.normalized_weights[:, None]
        sum_z = weighted_z.sum(axis=0)

        # Force balanced cluster weights
        self.pi = np.ones(self.k) / self.k  # Equal weight for each cluster

        # Update means
        self.mu = np.matmul(weighted_z.T, self.data)
        self.mu /= sum_z[:, None]

        # Update spherical covariance
        for i in range(self.k):
            diff = self.data - self.mu[i]  # Difference from cluster mean
            squared_distances = np.sum(diff**2, axis=1)  # Squared Euclidean distance
            variance = np.sum(weighted_z[:, i] * squared_distances) / (sum_z[i] * self.dim)
            self.sigma[i] = np.eye(self.dim) * variance  # Spherical covariance




    def log_likelihood(self, X):
        '''
        Compute the log-likelihood of X under current parameters
        input:
            - X: Data (batch_size, dim)
        output:
            - log-likelihood of X: Sum_n Sum_k log(pi_k * N( X_n | mu_k, sigma_k ))
        '''
        ll = []
        for d in X:
            tot = 0
            for i in range(self.k):
                tot += self.pi[i] * multivariate_normal.pdf(d, mean=self.mu[i], cov=self.sigma[i])
            ll.append(np.log(tot))
        return np.sum(ll)

    def plot_gaussian(self, mean, cov, ax, n_std=3.0, facecolor='none', **kwargs):
        '''
        Utility function to plot one Gaussian from mean and covariance.
        '''
        pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])
        ell_radius_x = np.sqrt(1 + pearson)
        ell_radius_y = np.sqrt(1 - pearson)
        ellipse = Ellipse((0, 0),
            width=ell_radius_x * 2,
            height=ell_radius_y * 2,
            facecolor=facecolor,
            **kwargs)
        scale_x = np.sqrt(cov[0, 0]) * n_std
        mean_x = mean[0]
        scale_y = np.sqrt(cov[1, 1]) * n_std
        mean_y = mean[1]
        transf = transforms.Affine2D() \
            .rotate_deg(45) \
            .scale(scale_x, scale_y) \
            .translate(mean_x, mean_y)
        ellipse.set_transform(transf + ax.transData)
        return ax.add_patch(ellipse)

    def draw(self, ax, n_std=2.0, facecolor='none', **kwargs):
        '''
        Function to draw the Gaussians.
        Note: Only for two-dimensionl dataset
        '''
        if(self.dim != 2):
            print("Drawing available only for 2D case.")
            return
        for i in range(self.k):
            self.plot_gaussian(self.mu[i], self.sigma[i], ax, n_std=n_std, edgecolor=self.colors[i], **kwargs)

def gen_data_random_with_weights(total_points, dim=2, lim=[-20, 20]):
    '''
    Generates random data with random weights assigned to each point.
    input:
        - k: Ignored but kept for compatibility.
        - dim: Dimension of generated points.
        - points_per_cluster: Total number of points to generate.
        - lim: Range for each dimension.
    output:
        - X: Randomly distributed points (points_per_cluster*k, dim).
        - weights: Array of weights (points_per_cluster*k,).
    '''

    X = np.random.uniform(low=lim[0], high=lim[1], size=(total_points, dim))
    weights = np.random.uniform(1, 100, size=total_points)  # Random weights between 1 and 100

    fig = plt.figure()
    ax = fig.gca()
    sc = ax.scatter(X[:, 0], X[:, 1], c=weights, s=3, alpha=0.4, cmap='viridis')
    plt.colorbar(sc, label="Weights")
    ax.autoscale(enable=True)
    plt.title("Randomly Distributed Data with Weights")
    return X, weights


def plot(title):
    '''
    Draw the data points with weights represented by circle sizes and clusters with different colors.
    input:
        - title: Title of plot and name with which it will be saved.
    '''
    fig = plt.figure(figsize=(8, 8))
    ax = fig.gca()

    # Assign each point to the most likely cluster
    cluster_assignments = np.argmax(gmm.z, axis=1)

    # Plot each cluster's points in a different color
    for cluster_idx in range(gmm.k):
        cluster_points = X[cluster_assignments == cluster_idx]
        cluster_weights = weights[cluster_assignments == cluster_idx]
        ax.scatter(
            cluster_points[:, 0], cluster_points[:, 1],
            s=cluster_weights,  # Scale size by weight
            c=[gmm.colors[cluster_idx]],  # Cluster color
            alpha=0.6,
            label=f"Cluster {cluster_idx + 1}"
        )

    # Plot Gaussian ellipses
    gmm.draw(ax, n_std=2.0, lw=2)

    # Add legend and title
    ax.set_xlim((-20, 20))
    ax.set_ylim((-20, 20))
    plt.title(title)
    plt.legend()
    plt.savefig(title.replace(':', '_'))
    plt.show()
    plt.clf()



# Calculate weight sum and point count for each cluster
def summarize_clusters():
    cluster_assignments = np.argmax(gmm.z, axis=1)  # Assign each point to the most likely cluster
    cluster_summary = []

    for cluster_idx in range(gmm.k):
        cluster_points = X[cluster_assignments == cluster_idx]
        cluster_weights = weights[cluster_assignments == cluster_idx]
        cluster_summary.append({
            "Cluster": cluster_idx + 1,
            "Number of Points": len(cluster_points),
            "Weight Sum": np.sum(cluster_weights)
        })

    # Create a DataFrame for better presentation
    summary_df = pd.DataFrame(cluster_summary)

    # Print the table to the console
    print(summary_df.to_string(index=False))



## Balanced training

# # Generate random 2D data with weights and 5 clusters
# X, weights = gen_data_random_with_weights(300, dim=2, lim=[-20, 20])


# Create a Gaussian Mixture Model with 5 clusters
gmm = GMM(k=4, dim=2)

# Initialize EM algorithm with weighted data
gmm.init_em(X, weights)

# Training with balanced EM
num_iters = 10
log_likelihood = [gmm.log_likelihood(X)]
plot("Iteration:  0")
for e in range(num_iters):
    gmm.e_step_balanced()  # Use balanced E-step
    gmm.m_step_balanced_spherical()  # Use balanced M-step
    log_likelihood.append(gmm.log_likelihood(X))
    print("Iteration: {}, log-likelihood: {:.4f}".format(e + 1, log_likelihood[-1]))
    plot(title="Iteration: " + str(e + 1))

# Summarize the clusters
summarize_clusters()

"""# **Shape to be spherical, balanced and at the same time less overlap :**"""

class GMM():
    def __init__(self, k, dim, init_mu=None, init_sigma=None, init_pi=None, colors=None):
        '''
        Define a model with known number of clusters and dimensions.
        input:
            - k: Number of Gaussian clusters
            - dim: Dimension
            - init_mu: initial value of mean of clusters (k, dim)
                       (default) random from uniform[-10, 10]
            - init_sigma: initial value of covariance matrix of clusters (k, dim, dim)
                          (default) Identity matrix for each cluster
            - init_pi: initial value of cluster weights (k,)
                       (default) equal value to all cluster i.e. 1/k
            - colors: Color valu for plotting each cluster (k, 3)
                      (default) random from uniform[0, 1]
        '''
        self.k = k
        self.dim = dim
        if(init_mu is None):
            init_mu = random.rand(k, dim)*20 - 10
        self.mu = init_mu
        if(init_sigma is None):
            init_sigma = np.zeros((k, dim, dim))
            for i in range(k):
                init_sigma[i] = np.eye(dim)
        self.sigma = init_sigma
        if(init_pi is None):
            init_pi = np.ones(self.k)/self.k
        self.pi = init_pi
        if(colors is None):
            colors = random.rand(k, 3)
        self.colors = colors

    def init_em(self, X, weights):
        '''
        Initialization for EM algorithm with weighted data.
        input:
            - X: Data (batch_size, dim).
            - weights: Array of weights for the data points.
        '''
        self.data = X
        self.weights = weights
        self.normalized_weights = weights / np.sum(weights) * len(weights)
        self.num_points = X.shape[0]
        self.z = np.zeros((self.num_points, self.k))
    def e_step(self):
        '''
        E-step of EM algorithm (with weights).
        '''
        for i in range(self.k):
            self.z[:, i] = self.pi[i] * multivariate_normal.pdf(self.data, mean=self.mu[i], cov=self.sigma[i])
        self.z *= self.normalized_weights[:, None]  # Weight each point's responsibility
        self.z /= self.z.sum(axis=1, keepdims=True)



    # For balanced weight constraints
    def e_step_balanced(self):
        '''
        E-step of EM algorithm with balanced weight constraints.
        '''
        avg_weight = np.sum(self.normalized_weights) / self.k  # Target balanced weight per cluster

        for i in range(self.k):
            # Compute the likelihood for each cluster
            self.z[:, i] = self.pi[i] * multivariate_normal.pdf(self.data, mean=self.mu[i], cov=self.sigma[i])

        # Adjust responsibilities to enforce balanced weights
        current_weights = np.sum(self.z * self.normalized_weights[:, None], axis=0)
        scaling_factors = avg_weight / (current_weights + 1e-9)  # Avoid division by zero
        self.z *= scaling_factors[None, :]  # Scale responsibilities
        self.z /= self.z.sum(axis=1, keepdims=True)  # Normalize






    def m_step(self):
        '''
        M-step of EM algorithm (with weights).
        '''
        weighted_z = self.z * self.normalized_weights[:, None]
        sum_z = weighted_z.sum(axis=0)
        self.pi = sum_z / np.sum(self.normalized_weights)
        self.mu = np.matmul(weighted_z.T, self.data)
        self.mu /= sum_z[:, None]
        for i in range(self.k):
            j = np.expand_dims(self.data, axis=1) - self.mu[i]
            s = np.matmul(j.transpose([0, 2, 1]), j)
            self.sigma[i] = np.matmul(s.transpose(1, 2, 0), weighted_z[:, i])
            self.sigma[i] /= sum_z[i]

    # For balanced weight constraints
    def m_step_balanced(self):
        '''
        M-step of EM algorithm with balanced weight constraints.
        '''
        weighted_z = self.z * self.normalized_weights[:, None]
        sum_z = weighted_z.sum(axis=0)

        # Force balanced cluster weights
        self.pi = np.ones(self.k) / self.k  # Equal weight for each cluster

        # Update means and covariances
        self.mu = np.matmul(weighted_z.T, self.data)
        self.mu /= sum_z[:, None]
        for i in range(self.k):
            j = np.expand_dims(self.data, axis=1) - self.mu[i]
            s = np.matmul(j.transpose([0, 2, 1]), j)
            self.sigma[i] = np.matmul(s.transpose(1, 2, 0), weighted_z[:, i])
            self.sigma[i] /= sum_z[i]

    # For balanced and spherical weight constraints
    def m_step_balanced_spherical(self):
        '''
        M-step of EM algorithm with balanced weight constraints and spherical clusters.
        '''
        weighted_z = self.z * self.normalized_weights[:, None]
        sum_z = weighted_z.sum(axis=0)

        # Force balanced cluster weights
        self.pi = np.ones(self.k) / self.k  # Equal weight for each cluster

        # Update means
        self.mu = np.matmul(weighted_z.T, self.data)
        self.mu /= sum_z[:, None]

        # Update spherical covariance
        for i in range(self.k):
            diff = self.data - self.mu[i]  # Difference from cluster mean
            squared_distances = np.sum(diff**2, axis=1)  # Squared Euclidean distance
            variance = np.sum(weighted_z[:, i] * squared_distances) / (sum_z[i] * self.dim)
            self.sigma[i] = np.eye(self.dim) * variance  # Spherical covariance

    def m_step_balanced_spherical_repulsion(self, repulsion_strength=0.1):
        '''
        M-step of EM algorithm with balanced weight constraints, spherical clusters, and reduced overlap.
        '''
        weighted_z = self.z * self.normalized_weights[:, None]
        sum_z = weighted_z.sum(axis=0)

        # Force balanced cluster weights
        self.pi = np.ones(self.k) / self.k  # Equal weight for each cluster

        # Update means
        self.mu = np.matmul(weighted_z.T, self.data)
        self.mu /= sum_z[:, None]

        # Apply repulsion to cluster centers
        for i in range(self.k):
            repulsion_force = np.zeros_like(self.mu[i])
            for j in range(self.k):
                if i != j:
                    diff = self.mu[i] - self.mu[j]
                    distance = np.linalg.norm(diff)
                    if distance > 1e-5:  # Avoid self-repulsion or numerical instability
                        repulsion_force += repulsion_strength * diff / (distance**3 + 1e-9)
            self.mu[i] += repulsion_force  # Adjust mean by repulsion force

        # Update spherical covariance
        for i in range(self.k):
            diff = self.data - self.mu[i]  # Difference from cluster mean
            squared_distances = np.sum(diff**2, axis=1)  # Squared Euclidean distance
            variance = np.sum(weighted_z[:, i] * squared_distances) / (sum_z[i] * self.dim)
            self.sigma[i] = np.eye(self.dim) * variance  # Spherical covariance



    def log_likelihood(self, X):
        '''
        Compute the log-likelihood of X under current parameters
        input:
            - X: Data (batch_size, dim)
        output:
            - log-likelihood of X: Sum_n Sum_k log(pi_k * N( X_n | mu_k, sigma_k ))
        '''
        ll = []
        for d in X:
            tot = 0
            for i in range(self.k):
                tot += self.pi[i] * multivariate_normal.pdf(d, mean=self.mu[i], cov=self.sigma[i])
            ll.append(np.log(tot))
        return np.sum(ll)

    def plot_gaussian(self, mean, cov, ax, n_std=3.0, facecolor='none', **kwargs):
        '''
        Utility function to plot one Gaussian from mean and covariance.
        '''
        pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])
        ell_radius_x = np.sqrt(1 + pearson)
        ell_radius_y = np.sqrt(1 - pearson)
        ellipse = Ellipse((0, 0),
            width=ell_radius_x * 2,
            height=ell_radius_y * 2,
            facecolor=facecolor,
            **kwargs)
        scale_x = np.sqrt(cov[0, 0]) * n_std
        mean_x = mean[0]
        scale_y = np.sqrt(cov[1, 1]) * n_std
        mean_y = mean[1]
        transf = transforms.Affine2D() \
            .rotate_deg(45) \
            .scale(scale_x, scale_y) \
            .translate(mean_x, mean_y)
        ellipse.set_transform(transf + ax.transData)
        return ax.add_patch(ellipse)

    def draw(self, ax, n_std=2.0, facecolor='none', **kwargs):
        '''
        Function to draw the Gaussians.
        Note: Only for two-dimensionl dataset
        '''
        if(self.dim != 2):
            print("Drawing available only for 2D case.")
            return
        for i in range(self.k):
            self.plot_gaussian(self.mu[i], self.sigma[i], ax, n_std=n_std, edgecolor=self.colors[i], **kwargs)

def gen_data_random_with_weights(total_points, dim=2, lim=[-20, 20]):
    '''
    Generates random data with random weights assigned to each point.
    input:
        - k: Ignored but kept for compatibility.
        - dim: Dimension of generated points.
        - points_per_cluster: Total number of points to generate.
        - lim: Range for each dimension.
    output:
        - X: Randomly distributed points (points_per_cluster*k, dim).
        - weights: Array of weights (points_per_cluster*k,).
    '''

    X = np.random.uniform(low=lim[0], high=lim[1], size=(total_points, dim))
    weights = np.random.uniform(1, 100, size=total_points)  # Random weights between 1 and 100

    fig = plt.figure()
    ax = fig.gca()
    sc = ax.scatter(X[:, 0], X[:, 1], c=weights, s=3, alpha=0.4, cmap='viridis')
    plt.colorbar(sc, label="Weights")
    ax.autoscale(enable=True)
    plt.title("Randomly Distributed Data with Weights")
    return X, weights


def plot(title):
    '''
    Draw the data points with weights represented by circle sizes and clusters with different colors.
    input:
        - title: Title of plot and name with which it will be saved.
    '''
    fig = plt.figure(figsize=(8, 8))
    ax = fig.gca()

    # Assign each point to the most likely cluster
    cluster_assignments = np.argmax(gmm.z, axis=1)

    # Plot each cluster's points in a different color
    for cluster_idx in range(gmm.k):
        cluster_points = X[cluster_assignments == cluster_idx]
        cluster_weights = weights[cluster_assignments == cluster_idx]
        ax.scatter(
            cluster_points[:, 0], cluster_points[:, 1],
            s=cluster_weights,  # Scale size by weight
            c=[gmm.colors[cluster_idx]],  # Cluster color
            alpha=0.6,
            label=f"Cluster {cluster_idx + 1}"
        )

    # Plot Gaussian ellipses
    gmm.draw(ax, n_std=2.0, lw=2)

    # Add legend and title
    ax.set_xlim((-20, 20))
    ax.set_ylim((-20, 20))
    plt.title(title)
    plt.legend()
    plt.savefig(title.replace(':', '_'))
    plt.show()
    plt.clf()



# Calculate weight sum and point count for each cluster
def summarize_clusters():
    cluster_assignments = np.argmax(gmm.z, axis=1)  # Assign each point to the most likely cluster
    cluster_summary = []

    for cluster_idx in range(gmm.k):
        cluster_points = X[cluster_assignments == cluster_idx]
        cluster_weights = weights[cluster_assignments == cluster_idx]
        cluster_summary.append({
            "Cluster": cluster_idx + 1,
            "Number of Points": len(cluster_points),
            "Weight Sum": np.sum(cluster_weights)
        })

    # Create a DataFrame for better presentation
    summary_df = pd.DataFrame(cluster_summary)

    # Print the table to the console
    print(summary_df.to_string(index=False))



## Balanced training

# # Generate random 2D data with weights and 5 clusters
# X, weights = gen_data_random_with_weights(300, dim=2, lim=[-20, 20])


# Create a Gaussian Mixture Model with 5 clusters
gmm = GMM(k=5, dim=2)

# Initialize EM algorithm with weighted data
gmm.init_em(X, weights)

# Training with balanced EM
num_iters = 10
log_likelihood = [gmm.log_likelihood(X)]
plot("Iteration:  0")
for e in range(num_iters):
    gmm.e_step_balanced()  # Use balanced E-step
    gmm.m_step_balanced_spherical_repulsion(repulsion_strength=10)  # Use balanced M-step
    log_likelihood.append(gmm.log_likelihood(X))
    print("Iteration: {}, log-likelihood: {:.4f}".format(e + 1, log_likelihood[-1]))
    plot(title="Iteration: " + str(e + 1))

# Summarize the clusters
summarize_clusters()

"""# **The previous one does not work very good**"""

class GMM():
    def __init__(self, k, dim, init_mu=None, init_sigma=None, init_pi=None, colors=None):
        '''
        Define a model with known number of clusters and dimensions.
        input:
            - k: Number of Gaussian clusters
            - dim: Dimension
            - init_mu: initial value of mean of clusters (k, dim)
                       (default) random from uniform[-10, 10]
            - init_sigma: initial value of covariance matrix of clusters (k, dim, dim)
                          (default) Identity matrix for each cluster
            - init_pi: initial value of cluster weights (k,)
                       (default) equal value to all cluster i.e. 1/k
            - colors: Color valu for plotting each cluster (k, 3)
                      (default) random from uniform[0, 1]
        '''
        self.k = k
        self.dim = dim
        if(init_mu is None):
            init_mu = random.rand(k, dim)*20 - 10
        self.mu = init_mu
        if(init_sigma is None):
            init_sigma = np.zeros((k, dim, dim))
            for i in range(k):
                init_sigma[i] = np.eye(dim)
        self.sigma = init_sigma
        if(init_pi is None):
            init_pi = np.ones(self.k)/self.k
        self.pi = init_pi
        if(colors is None):
            colors = random.rand(k, 3)
        self.colors = colors

    def init_em(self, X, weights):
        '''
        Initialization for EM algorithm with weighted data.
        input:
            - X: Data (batch_size, dim).
            - weights: Array of weights for the data points.
        '''
        self.data = X
        self.weights = weights
        self.normalized_weights = weights / np.sum(weights) * len(weights)
        self.num_points = X.shape[0]
        self.z = np.zeros((self.num_points, self.k))
    def e_step(self):
        '''
        E-step of EM algorithm (with weights).
        '''
        for i in range(self.k):
            self.z[:, i] = self.pi[i] * multivariate_normal.pdf(self.data, mean=self.mu[i], cov=self.sigma[i])
        self.z *= self.normalized_weights[:, None]  # Weight each point's responsibility
        self.z /= self.z.sum(axis=1, keepdims=True)



    # For balanced weight constraints
    def e_step_balanced(self):
        '''
        E-step of EM algorithm with balanced weight constraints.
        '''
        avg_weight = np.sum(self.normalized_weights) / self.k  # Target balanced weight per cluster

        for i in range(self.k):
            # Compute the likelihood for each cluster
            self.z[:, i] = self.pi[i] * multivariate_normal.pdf(self.data, mean=self.mu[i], cov=self.sigma[i])

        # Adjust responsibilities to enforce balanced weights
        current_weights = np.sum(self.z * self.normalized_weights[:, None], axis=0)
        scaling_factors = avg_weight / (current_weights + 1e-9)  # Avoid division by zero
        self.z *= scaling_factors[None, :]  # Scale responsibilities
        self.z /= self.z.sum(axis=1, keepdims=True)  # Normalize






    def m_step(self):
        '''
        M-step of EM algorithm (with weights).
        '''
        weighted_z = self.z * self.normalized_weights[:, None]
        sum_z = weighted_z.sum(axis=0)
        self.pi = sum_z / np.sum(self.normalized_weights)
        self.mu = np.matmul(weighted_z.T, self.data)
        self.mu /= sum_z[:, None]
        for i in range(self.k):
            j = np.expand_dims(self.data, axis=1) - self.mu[i]
            s = np.matmul(j.transpose([0, 2, 1]), j)
            self.sigma[i] = np.matmul(s.transpose(1, 2, 0), weighted_z[:, i])
            self.sigma[i] /= sum_z[i]

    # For balanced weight constraints
    def m_step_balanced(self):
        '''
        M-step of EM algorithm with balanced weight constraints.
        '''
        weighted_z = self.z * self.normalized_weights[:, None]
        sum_z = weighted_z.sum(axis=0)

        # Force balanced cluster weights
        self.pi = np.ones(self.k) / self.k  # Equal weight for each cluster

        # Update means and covariances
        self.mu = np.matmul(weighted_z.T, self.data)
        self.mu /= sum_z[:, None]
        for i in range(self.k):
            j = np.expand_dims(self.data, axis=1) - self.mu[i]
            s = np.matmul(j.transpose([0, 2, 1]), j)
            self.sigma[i] = np.matmul(s.transpose(1, 2, 0), weighted_z[:, i])
            self.sigma[i] /= sum_z[i]

    # For balanced and spherical weight constraints
    def m_step_balanced_spherical(self):
        '''
        M-step of EM algorithm with balanced weight constraints and spherical clusters.
        '''
        weighted_z = self.z * self.normalized_weights[:, None]
        sum_z = weighted_z.sum(axis=0)

        # Force balanced cluster weights
        self.pi = np.ones(self.k) / self.k  # Equal weight for each cluster

        # Update means
        self.mu = np.matmul(weighted_z.T, self.data)
        self.mu /= sum_z[:, None]

        # Update spherical covariance
        for i in range(self.k):
            diff = self.data - self.mu[i]  # Difference from cluster mean
            squared_distances = np.sum(diff**2, axis=1)  # Squared Euclidean distance
            variance = np.sum(weighted_z[:, i] * squared_distances) / (sum_z[i] * self.dim)
            self.sigma[i] = np.eye(self.dim) * variance  # Spherical covariance

    def m_step_balanced_spherical_repulsion(self, repulsion_strength=0.1):
        '''
        M-step of EM algorithm with balanced weight constraints, spherical clusters, and reduced overlap.
        '''
        weighted_z = self.z * self.normalized_weights[:, None]
        sum_z = weighted_z.sum(axis=0)

        # Force balanced cluster weights
        self.pi = np.ones(self.k) / self.k  # Equal weight for each cluster

        # Update means
        self.mu = np.matmul(weighted_z.T, self.data)
        self.mu /= sum_z[:, None]

        # Apply repulsion to cluster centers
        for i in range(self.k):
            repulsion_force = np.zeros_like(self.mu[i])
            for j in range(self.k):
                if i != j:
                    diff = self.mu[i] - self.mu[j]
                    distance = np.linalg.norm(diff)
                    if distance > 1e-5:  # Avoid self-repulsion or numerical instability
                        repulsion_force += repulsion_strength * diff / (distance**3 + 1e-9)
            self.mu[i] += repulsion_force  # Adjust mean by repulsion force

        # Update spherical covariance
        for i in range(self.k):
            diff = self.data - self.mu[i]  # Difference from cluster mean
            squared_distances = np.sum(diff**2, axis=1)  # Squared Euclidean distance
            variance = np.sum(weighted_z[:, i] * squared_distances) / (sum_z[i] * self.dim)
            self.sigma[i] = np.eye(self.dim) * variance  # Spherical covariance

    def m_step_with_penalty(self, penalty_strength=0.1):
        '''
        M-step with penalty for overlap minimization.
        '''
        weighted_z = self.z * self.normalized_weights[:, None]
        sum_z = weighted_z.sum(axis=0)

        # Force balanced cluster weights
        self.pi = np.ones(self.k) / self.k  # Equal weight for each cluster

        # Update means
        self.mu = np.matmul(weighted_z.T, self.data)
        self.mu /= sum_z[:, None]

        # Apply penalty-based repulsion to means
        for i in range(self.k):
            repulsion_force = np.zeros_like(self.mu[i])
            for j in range(self.k):
                if i != j:
                    diff = self.mu[i] - self.mu[j]
                    distance = np.linalg.norm(diff)
                    if distance > 1e-5:  # Avoid self-repulsion or numerical instability
                        combined_variance = np.trace(self.sigma[i]) + np.trace(self.sigma[j])
                        repulsion_force += penalty_strength * diff * np.exp(-distance**2 / (2 * combined_variance))
            self.mu[i] += repulsion_force  # Adjust mean by repulsion force

        # Update spherical covariance
        for i in range(self.k):
            diff = self.data - self.mu[i]  # Difference from cluster mean
            squared_distances = np.sum(diff**2, axis=1)  # Squared Euclidean distance
            variance = np.sum(weighted_z[:, i] * squared_distances) / (sum_z[i] * self.dim)
            self.sigma[i] = np.eye(self.dim) * variance  # Spherical covariance


    def log_likelihood(self, X):
        '''
        Compute the log-likelihood of X under current parameters
        input:
            - X: Data (batch_size, dim)
        output:
            - log-likelihood of X: Sum_n Sum_k log(pi_k * N( X_n | mu_k, sigma_k ))
        '''
        ll = []
        for d in X:
            tot = 0
            for i in range(self.k):
                tot += self.pi[i] * multivariate_normal.pdf(d, mean=self.mu[i], cov=self.sigma[i])
            ll.append(np.log(tot))
        return np.sum(ll)
    def log_likelihood_with_penalty(self, X, penalty_strength=0.1):
        '''
        Compute the adjusted log-likelihood of X with overlap penalty.
        input:
            - X: Data (batch_size, dim)
            - penalty_strength: Weight of the overlap penalty
        output:
            - Adjusted log-likelihood
        '''
        # Standard log-likelihood
        ll = []
        for d in X:
            tot = 0
            for i in range(self.k):
                tot += self.pi[i] * multivariate_normal.pdf(d, mean=self.mu[i], cov=self.sigma[i])
            ll.append(np.log(tot))
        log_likelihood = np.sum(ll)

        # Overlap penalty
        penalty = 0
        for i in range(self.k):
            for j in range(i + 1, self.k):
                diff = np.linalg.norm(self.mu[i] - self.mu[j])
                combined_variance = np.trace(self.sigma[i]) + np.trace(self.sigma[j])  # Sum of variances
                penalty += np.exp(-diff**2 / (2 * combined_variance))

        # Adjusted log-likelihood
        return log_likelihood - penalty_strength * penalty

    def plot_gaussian(self, mean, cov, ax, n_std=3.0, facecolor='none', **kwargs):
        '''
        Utility function to plot one Gaussian from mean and covariance.
        '''
        pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])
        ell_radius_x = np.sqrt(1 + pearson)
        ell_radius_y = np.sqrt(1 - pearson)
        ellipse = Ellipse((0, 0),
            width=ell_radius_x * 2,
            height=ell_radius_y * 2,
            facecolor=facecolor,
            **kwargs)
        scale_x = np.sqrt(cov[0, 0]) * n_std
        mean_x = mean[0]
        scale_y = np.sqrt(cov[1, 1]) * n_std
        mean_y = mean[1]
        transf = transforms.Affine2D() \
            .rotate_deg(45) \
            .scale(scale_x, scale_y) \
            .translate(mean_x, mean_y)
        ellipse.set_transform(transf + ax.transData)
        return ax.add_patch(ellipse)

    def draw(self, ax, n_std=2.0, facecolor='none', **kwargs):
        '''
        Function to draw the Gaussians.
        Note: Only for two-dimensionl dataset
        '''
        if(self.dim != 2):
            print("Drawing available only for 2D case.")
            return
        for i in range(self.k):
            self.plot_gaussian(self.mu[i], self.sigma[i], ax, n_std=n_std, edgecolor=self.colors[i], **kwargs)



def gen_data_random_with_weights(total_points, dim=2, lim=[-20, 20]):
    '''
    Generates random data with random weights assigned to each point.
    input:
        - k: Ignored but kept for compatibility.
        - dim: Dimension of generated points.
        - points_per_cluster: Total number of points to generate.
        - lim: Range for each dimension.
    output:
        - X: Randomly distributed points (points_per_cluster*k, dim).
        - weights: Array of weights (points_per_cluster*k,).
    '''

    X = np.random.uniform(low=lim[0], high=lim[1], size=(total_points, dim))
    weights = np.random.uniform(1, 100, size=total_points)  # Random weights between 1 and 100

    fig = plt.figure()
    ax = fig.gca()
    sc = ax.scatter(X[:, 0], X[:, 1], c=weights, s=3, alpha=0.4, cmap='viridis')
    plt.colorbar(sc, label="Weights")
    ax.autoscale(enable=True)
    plt.title("Randomly Distributed Data with Weights")
    return X, weights


def plot(title):
    '''
    Draw the data points with weights represented by circle sizes and clusters with different colors.
    input:
        - title: Title of plot and name with which it will be saved.
    '''
    fig = plt.figure(figsize=(8, 8))
    ax = fig.gca()

    # Assign each point to the most likely cluster
    cluster_assignments = np.argmax(gmm.z, axis=1)

    # Plot each cluster's points in a different color
    for cluster_idx in range(gmm.k):
        cluster_points = X[cluster_assignments == cluster_idx]
        cluster_weights = weights[cluster_assignments == cluster_idx]
        ax.scatter(
            cluster_points[:, 0], cluster_points[:, 1],
            s=cluster_weights,  # Scale size by weight
            c=[gmm.colors[cluster_idx]],  # Cluster color
            alpha=0.6,
            label=f"Cluster {cluster_idx + 1}"
        )

    # Plot Gaussian ellipses
    gmm.draw(ax, n_std=2.0, lw=2)

    # Add legend and title
    ax.set_xlim((-20, 20))
    ax.set_ylim((-20, 20))
    plt.title(title)
    plt.legend()
    plt.savefig(title.replace(':', '_'))
    plt.show()
    plt.clf()



# Calculate weight sum and point count for each cluster
def summarize_clusters():
    cluster_assignments = np.argmax(gmm.z, axis=1)  # Assign each point to the most likely cluster
    cluster_summary = []

    for cluster_idx in range(gmm.k):
        cluster_points = X[cluster_assignments == cluster_idx]
        cluster_weights = weights[cluster_assignments == cluster_idx]
        cluster_summary.append({
            "Cluster": cluster_idx + 1,
            "Number of Points": len(cluster_points),
            "Weight Sum": np.sum(cluster_weights)
        })

    # Create a DataFrame for better presentation
    summary_df = pd.DataFrame(cluster_summary)

    # Print the table to the console
    print(summary_df.to_string(index=False))



## Balanced training

# # Generate random 2D data with weights and 5 clusters
# X, weights = gen_data_random_with_weights(300, dim=2, lim=[-20, 20])


# Create a Gaussian Mixture Model with 5 clusters
gmm = GMM(k=5, dim=2)

# Initialize EM algorithm with weighted data
gmm.init_em(X, weights)

# Training with balanced EM
num_iters = 10
log_likelihood = [gmm.log_likelihood_with_penalty(X, penalty_strength=5)]
plot("Iteration:  0")
for e in range(num_iters):
    gmm.e_step_balanced()  # Use balanced E-step
    gmm.m_step_with_penalty(penalty_strength=0.1)  # Penalty-based M-step
    log_likelihood.append(gmm.log_likelihood_with_penalty(X, penalty_strength=5))
    print("Iteration: {}, log-likelihood: {:.4f}".format(e + 1, log_likelihood[-1]))
    plot(title="Iteration: " + str(e + 1))

# Summarize the clusters
summarize_clusters()

import pandas as pd
import matplotlib.pyplot as plt
def k_means_balanced(X, weights, k, max_iters=100, tolerance=1e-4, random_state=None):
    """
    K-Means clustering with balanced weights.

    Parameters:
        - X: Data points (n_samples, n_features)
        - weights: Weight of each point (n_samples,)
        - k: Number of clusters
        - max_iters: Maximum number of iterations
        - tolerance: Convergence tolerance for centroids
        - random_state: Random seed for initialization

    Returns:
        - centroids: Final cluster centroids (k, n_features)
        - assignments: Cluster assignment for each point (n_samples,)
    """
    if random_state is not None:
        np.random.seed(random_state)

    n_samples, n_features = X.shape
    # Normalize weights
    normalized_weights = weights / np.sum(weights)

    # Initialize centroids randomly from the data
    centroids = X[np.random.choice(n_samples, k, replace=False)]

    assignments = np.zeros(n_samples, dtype=int)
    for iteration in range(max_iters):
        # Assignment step: Assign points to nearest centroids
        distances = np.linalg.norm(X[:, None] - centroids, axis=2)  # Pairwise distances to centroids
        sorted_indices = np.argsort(distances, axis=1)  # Sort points by closest centroids

        # Balanced assignment
        cluster_weights = np.zeros(k)
        new_assignments = np.full(n_samples, -1, dtype=int)
        for point_idx, sorted_cluster_indices in enumerate(sorted_indices):
            for cluster_idx in sorted_cluster_indices:
                if cluster_weights[cluster_idx] + normalized_weights[point_idx] <= 1 / k:
                    new_assignments[point_idx] = cluster_idx
                    cluster_weights[cluster_idx] += normalized_weights[point_idx]
                    break

        # Update step: Recompute centroids as weighted averages
        new_centroids = np.zeros((k, n_features))
        for cluster_idx in range(k):
            cluster_points = X[new_assignments == cluster_idx]
            cluster_weights_points = weights[new_assignments == cluster_idx]
            if len(cluster_points) > 0:
                new_centroids[cluster_idx] = np.average(cluster_points, axis=0, weights=cluster_weights_points)
            else:
                new_centroids[cluster_idx] = centroids[cluster_idx]  # Retain old centroid if no points assigned

        # Check for convergence
        if np.linalg.norm(new_centroids - centroids) < tolerance:
            break

        centroids = new_centroids
        assignments = new_assignments

    return centroids, assignments



def gen_data_random_with_weights(k=3, dim=2, points_per_cluster=200, lim=[-10, 10]):
    '''
    Generates random data with random weights assigned to each point.
    input:
        - k: Ignored but kept for compatibility.
        - dim: Dimension of generated points.
        - points_per_cluster: Total number of points to generate.
        - lim: Range for each dimension.
    output:
        - X: Randomly distributed points (points_per_cluster*k, dim).
        - weights: Array of weights (points_per_cluster*k,).
    '''
    total_points = points_per_cluster * k
    X = np.random.uniform(low=lim[0], high=lim[1], size=(total_points, dim))
    weights = np.random.uniform(1, 100, size=total_points)  # Random weights between 1 and 100
    if dim == 2:
        fig = plt.figure()
        ax = fig.gca()
        sc = ax.scatter(X[:, 0], X[:, 1], c=weights, s=3, alpha=0.4, cmap='viridis')
        plt.colorbar(sc, label="Weights")
        ax.autoscale(enable=True)
        plt.title("Randomly Distributed Data with Weights")
    return X, weights



def draw_circles_and_table(X, weights, centroids, assignments, k):
    """
    Draws circles centered at centroids with radii proportional to the cluster size.
    Displays a table summarizing the weight sum and number of points in each cluster.

    Parameters:
        - X: Data points (n_samples, n_features)
        - weights: Weight of each point (n_samples,)
        - centroids: Cluster centroids (k, n_features)
        - assignments: Cluster assignment for each point (n_samples,)
        - k: Number of clusters
    """
    # Prepare the figure
    fig, ax = plt.subplots(figsize=(8, 8))

    # Cluster summary data
    cluster_summary = []

    # Draw clusters
    for cluster_idx in range(k):
        cluster_points = X[assignments == cluster_idx]
        cluster_weights = weights[assignments == cluster_idx]
        cluster_weight_sum = np.sum(cluster_weights)
        num_points = len(cluster_points)

        # Calculate radius proportional to the number of points
        radius = np.sqrt(num_points / np.pi)
        # Add circle around centroid
        circle = plt.Circle(
            (centroids[cluster_idx, 0], centroids[cluster_idx, 1]),
            radius=radius,
            color=f"C{cluster_idx}",
            alpha=0.3,
            label=f"Cluster {cluster_idx + 1}"
        )
        ax.add_patch(circle)

        # Scatter points
        ax.scatter(
            cluster_points[:, 0],
            cluster_points[:, 1],
            s=weights[assignments == cluster_idx],  # Scale size by weight
            alpha=0.6,
            label=f"Cluster {cluster_idx + 1} Points"
        )

        # Add to cluster summary
        cluster_summary.append({
            "Cluster": cluster_idx + 1,
            "Number of Points": num_points,
            "Weight Sum": cluster_weight_sum
        })

    # Plot centroids
    ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=200, label='Centroids')

    # Add legend and title
    ax.legend()
    ax.set_title("K-Means Clustering with Circles Representing Cluster Sizes")
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.axis('equal')
    plt.show()

    # Create a DataFrame for the cluster summary
    summary_df = pd.DataFrame(cluster_summary)

    # Print the summary table
    print(summary_df.to_string(index=False))


# Generate random 2D data with weights and 5 clusters
X, weights = gen_data_random_with_weights(k=3, dim=2, points_per_cluster=100)

# Number of clusters
k = 3

# Run K-Means with balanced weights
centroids, assignments = k_means_balanced(X, weights, k, max_iters=100, random_state=42)


# Visualize results
import matplotlib.pyplot as plt


# Draw circles and show the table
draw_circles_and_table(X, weights, centroids, assignments, k)



plt.figure(figsize=(8, 8))
for cluster_idx in range(k):
    cluster_points = X[assignments == cluster_idx]
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], s=weights[assignments == cluster_idx], alpha=0.6, label=f"Cluster {cluster_idx + 1}")
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=200, label='Centroids')
plt.legend()
plt.title("K-Means with Balanced Weights")
plt.show()